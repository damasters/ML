{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Daniel Masters ||\n",
    "GitHub username: damasters ||\n",
    "USC ID: 6203966352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DSCI 552 Homework #1 </h1>\n",
    "<h2> 1. Vertebral Column Data Set</h2>\n",
    "<p1> This Biomedical data set was built by Dr. Henrique da Mota during a medical residence\n",
    "period in Lyon, France. Each patient in the data set is represented in the data set\n",
    "by six biomechanical attributes derived from the shape and orientation of the pelvis\n",
    "and lumbar spine (in this order): pelvic incidence, pelvic tilt, lumbar lordosis angle,\n",
    "sacral slope, pelvic radius and grade of spondylolisthesis. The following convention is\n",
    "used for the class labels: DH (Disk Hernia), Spondylolisthesis (SL), Normal (NO) and\n",
    "Abnormal (AB). In this exercise, we only focus on a binary classification task NO=0\n",
    "and AB=1. </p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "Download the Vertebral Column Data Set from:\n",
    "https://archive.ics.uci.edu/ml/datasets/Vertebral+Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import math\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_file = '../data/vertebral_column_data/column_2C.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/vertebral_column_data/column_2C.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1914215508aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdat_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/vertebral_column_data/column_2C.dat'"
     ]
    }
   ],
   "source": [
    "f = open(dat_file, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_table(dat_file, header= None, delimiter = \"\\s+\")\n",
    "d.columns = ['pelvic incidence', 'pelvic tilt', 'lumbar lordosis angle', 'sacral slope', 'pelvic radius', 'grade of spondylolisthesis', 'class'] \n",
    "df = pd.DataFrame(data=d)\n",
    "df['class'] = df['class'].replace('AB', 1)\n",
    "df['class'] = df['class'].replace('NO', 0)\n",
    "df.to_csv('DSCI_552_HW1_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('DSCI_552_HW1_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Pre-Processing and Exploratory data analysis:\n",
    " i) Make scatterplots of the independent variables in the dataset. Use color to\n",
    "    show Classes 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(df_csv, hue= 'class', vars= ['pelvic incidence', 'pelvic tilt', 'lumbar lordosis angle', 'sacral slope', 'pelvic radius', 'grade of spondylolisthesis'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)ii) Make boxplots for each of the independent variables. Use color to show\n",
    "Classes 0 and 1 (see ISLR p. 129)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x= 'class', y= 'pelvic incidence', data= df_csv, hue= 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x= 'class', y= 'pelvic tilt', data= df_csv, hue= 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x= 'class', y= 'lumbar lordosis angle', data= df_csv, hue= 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x= 'class', y= 'sacral slope', data= df_csv, hue= 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x= 'class', y= 'pelvic radius', data= df_csv, hue= 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x= 'class', y= 'grade of spondylolisthesis', data= df_csv, hue= 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)iii)\n",
    "Select the first 70 rows of Class 0 and the first 140 rows of Class 1 as the\n",
    "training set and the rest of the data as the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_0 = df[df['class'] == 0].iloc[:70, :]\n",
    "train_class_1 = df[df['class'] == 1].iloc[:140, :]\n",
    "dfs = [train_class_0, train_class_1]\n",
    "train_set = pd.concat(dfs)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training sets with and without labels to feed algorithms\n",
    "train_set_x = train_set.iloc[:, 0:6]\n",
    "train_set_y = train_set.iloc[:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_vals_0 = df[df['class'] == 0].iloc[70:, :]\n",
    "other_vals_1 = df[df['class'] == 1].iloc[140:, :]\n",
    "dfs_2 = [other_vals_0, other_vals_1]\n",
    "test_set = pd.concat(dfs_2)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test sets with and without labels to feed algorithms\n",
    "test_set_x = test_set.iloc[:, :6]\n",
    "test_set_y = test_set.iloc[:, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Classification using KNN on Vertebral Column Data Set\n",
    "\n",
    "c)i) Write code for k-nearest neighbors with Euclidean metric (or use a software\n",
    "package).\n",
    "\n",
    "c)ii) Test all the data in the test database with k nearest neighbors. Take decisions by majority polling. Plot train and test errors in terms of k for k ∈ {208, 205, . . . , 7, 4, 1, } (in reverse order). You are welcome to use smaller increments of k. Which k* is the most suitable k among those values? Calculate the confusion matrix, true positive rate, true negative rate, precision, and F-score when k = k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = {}\n",
    "for k in range(208, 0, -1):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k) #210 training points\n",
    "    knn.fit(train_set_x, train_set_y)    #fit to training data\n",
    "    pred_k = knn.predict(test_set_x)     #prediction with test data\n",
    "    pred_acc = accuracy_score(test_set_y, pred_k)*100\n",
    "    acc_list[k] = pred_acc\n",
    "best_k = max(acc_list, key= acc_list.get)\n",
    "best_acc = max(acc_list.values())\n",
    "print(\"The most suitable K is:\",str(best_k),\"\\nIt's accuracy is: \"+str(best_acc)+\"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a confusion matrix with k=4\n",
    "knn_4= KNeighborsClassifier(n_neighbors= 4)\n",
    "knn_4.fit(train_set_x, train_set_y)\n",
    "pred_k_4= knn_4.predict(test_set_x)\n",
    "conf_matrix= confusion_matrix(test_set_y, pred_k_4)\n",
    "print(\"Confusion Matrix calculation:\\n\"+str(conf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the precision, recall, f1-score\n",
    "print(classification_report(test_set_y, pred_k_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating true positive rate = TP/TP+FN\n",
    "tp = conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[1,0])\n",
    "tp_percent = tp*100\n",
    "print(\"The true positive rate is: \"+str(tp)+\" or \"+str(tp_percent)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating true negative rate = TN/FP+TN\n",
    "tn = conf_matrix[1,1] / (conf_matrix[0,1] + conf_matrix[1,1])\n",
    "tn_percent = tn*100\n",
    "print(\"The true negative rate is: \"+str(tn)+\" or \"+str(tn_percent)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the precision = TP/TP+FP\n",
    "precision = conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[0,1])\n",
    "prec_percent = precision*100\n",
    "print(\"The precision is: \"+str(precision)+\" or \"+str(prec_percent)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the F1-score = TP / (TP + .5(FP + FN))\n",
    "f1_score = conf_matrix[0,0] / (conf_matrix[0,0] + .5*(conf_matrix[0,1] + conf_matrix[1,0]))\n",
    "print(\"The f1-score is: \"+str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting train and test errors of k\n",
    "x_axis = []\n",
    "tr_error_list = []\n",
    "te_error_list = []\n",
    "for k in range(210, 0, -1):\n",
    "    x_axis.append(k)\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(train_set_x, train_set_y)\n",
    "    pred_train = knn.predict(train_set_x)\n",
    "    pred_test = knn.predict(test_set_x)\n",
    "    training_error = (1 - accuracy_score(train_set_y, pred_train))\n",
    "    tr_error_list.append(training_error)\n",
    "    test_error = (1 - accuracy_score(test_set_y, pred_test))\n",
    "    te_error_list.append(test_error)\n",
    "plt.plot(x_axis, tr_error_list, label = \"Training Error\")\n",
    "plt.plot(x_axis, te_error_list, label = \"Test Error\")\n",
    "plt.title(\"Training and Test Errors at Each K\")\n",
    "plt.xlabel(\"Number of Neighbors at Each K\")\n",
    "plt.ylabel(\"Error (in Hundredths)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)iii) Since the computation time depends on the size of the training set, one may only use a subset of the training set. Plot the best test error rate, which is obtained by some value of k, against the size of training set, when the size of training set is N ∈ {10, 20, 30, . . . , 210}. Note: for each N, select your training set by choosing the ﬁrst N/3 rows of Class 0 and the ﬁrst N − N/3 rows of Class 1 in the training set you created in 1(b)iii. Also, for each N, select the optimal k from a set starting from k = 1, increasing by 5. For example, if N = 200, the optimal k is selected from {1, 6, 11, . . . , 196}. This plot is called a Learning Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = []\n",
    "best_error_list = []\n",
    "for n in range(10, 211, 10):\n",
    "    n_list.append(n)\n",
    "    calc_0= math.floor(n/3)\n",
    "    calc_1= n - (math.floor(n/3))\n",
    "    train_class_0_sub= train_class_0.iloc[:calc_0]\n",
    "    train_class_1_sub= train_class_1.iloc[:calc_1]\n",
    "    combined= pd.concat([train_class_0_sub, train_class_1_sub])\n",
    "    new_train_x= combined.iloc[:, 0:6]\n",
    "    new_train_y= combined.iloc[:, 6]\n",
    "    optimal_k = []\n",
    "    for k in range(1, n, 5):\n",
    "        knn_k= KNeighborsClassifier(n_neighbors= k)\n",
    "        knn_k.fit(new_train_x, new_train_y)\n",
    "        new_pred= knn_k.predict(test_set_x)\n",
    "        new_acc= accuracy_score(test_set_y, new_pred)\n",
    "        error_rate= 1-new_acc\n",
    "        optimal_k.append(error_rate)\n",
    "    best_error= min(optimal_k)\n",
    "    best_error_list.append(best_error)\n",
    "plt.plot(n_list, best_error_list, label=\"Best Test Error Rate\")\n",
    "plt.title(\"Plotting Best Test Error Rate\")\n",
    "plt.xlabel(\"Size of Training Set (N)\")\n",
    "plt.ylabel(\"Best Test Error Rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Replace the Euclidean metric with the following metrics and test them. Summarize the test errors (i.e., when k = k∗) in a table. Use all of your training data and select the best k when {1, 6, 11, . . . , 196}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)i) Minkowski Distance:\n",
    "    A. which becomes Manhattan Distance with p= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_dict = {}\n",
    "for k in range(1, 197, 5):\n",
    "    knn = KNeighborsClassifier(n_neighbors= k, p= 1) #210 training points\n",
    "    knn.fit(train_set_x, train_set_y)    #fit to training data\n",
    "    pred_k = knn.predict(test_set_x)     #prediction with test data\n",
    "    pred_acc = accuracy_score(test_set_y, pred_k)\n",
    "    pred_error= 1-pred_acc\n",
    "    error_dict[k] = pred_error\n",
    "k_best = min(error_dict, key= error_dict.get)\n",
    "best_error_rate = min(error_dict.values())\n",
    "print(\"The best K is:\",str(k_best),\"\\nIt's best error rate is: \"+str(best_error_rate)+\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)i)B) with log(10)(p) ∈ {0.1, 0.2, 0.3, . . . , 1}. In this case, use the k∗ you found for the Manhattan distance in 1(d)iA. What is the best log(10)(p)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_6_errdict={}\n",
    "for p in np.arange(0.1, 1.1, 0.1):\n",
    "    knn_6= KNeighborsClassifier(n_neighbors= 6, p= pow(10,p))\n",
    "    knn_6.fit(train_set_x, train_set_y)\n",
    "    knn_6_predict= knn_6.predict(test_set_x)\n",
    "    knn_6_acc= accuracy_score(test_set_y, knn_6_predict)\n",
    "    knn_6_error= 1-knn_6_acc\n",
    "    knn_6_errdict[p] = knn_6_error\n",
    "best_p = min(knn_6_errdict, key= knn_6_errdict.get)\n",
    "knn_6_best = min(knn_6_errdict.values())\n",
    "print(\"The best log10(p) is:\",str(best_p),\"\\nIt's best error rate is: \"+str(knn_6_best)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)i)C) which becomes Chebyshev Distance with p→ ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_errdict = {}\n",
    "for i in range(1, 197, 5):\n",
    "    knn= KNeighborsClassifier(n_neighbors= i, metric= 'chebyshev')\n",
    "    knn.fit(train_set_x, train_set_y)\n",
    "    knn_predict= knn.predict(test_set_x)\n",
    "    knn_acc= accuracy_score(test_set_y, knn_predict)\n",
    "    knn_error= 1-knn_acc\n",
    "    knn_errdict[i] = knn_error\n",
    "best_cheby_err = min(knn_errdict.values())\n",
    "mult_k = []\n",
    "for k, v in knn_errdict.items():\n",
    "    if v == best_cheby_err:\n",
    "        mult_k.append(k)\n",
    "best_cheby_k = max(mult_k)\n",
    "print(\"The best k is:\",str(best_cheby_k),\"\\nIt's best error rate is: \"+str(best_cheby_err)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)ii) Mahalanobis Distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_errdict= {}\n",
    "cov_train= train_set_x.cov()\n",
    "pseudo_inv= np.linalg.pinv(cov_train)\n",
    "for i in range(1, 197, 5):\n",
    "    knn= KNeighborsClassifier(n_neighbors= i, metric= 'mahalanobis', metric_params= {'VI': pseudo_inv})\n",
    "    knn.fit(train_set_x, train_set_y)\n",
    "    knn_predict= knn.predict(test_set_x)\n",
    "    knn_acc= accuracy_score(test_set_y, knn_predict)\n",
    "    knn_error= 1-knn_acc\n",
    "    knn_errdict[i] = knn_error\n",
    "best_mahala_err = min(knn_errdict.values())\n",
    "many_k = []\n",
    "for k, v in knn_errdict.items():\n",
    "    if v == best_mahala_err:\n",
    "        many_k.append(k)\n",
    "best_mahala_k = max(many_k)\n",
    "print(\"The best k is:\",str(best_mahala_k),\"\\nIt's best error rate is: \"+str(best_mahala_err)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_format = {'Distance Measurements':['Manhattan Distance (p=1)', 'log10(p)', 'Chebyshev Distance', 'Mahalanobis'], 'Best K (or P)':[k_best, best_p, best_cheby_k, best_mahala_k], 'Best Test Error': [best_error_rate, knn_6_best, best_cheby_err, best_mahala_err]}\n",
    "table = pd.DataFrame(table_format)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) The majority polling decision can be replaced by weighted decision, in which the weight of each point in voting is inversely proportional to its distance from the query/test data point. In this case, closer neighbors of a query point will have a greater inﬂuence than neighbors which are further away. Use weighted voting with Euclidean, Manhattan, and Chebyshev distances and report the best test errors when k ∈ {1, 6, 11, 16, . . . , 196}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclid_dict= {}\n",
    "for i in range(1, 197, 5):\n",
    "    knn= KNeighborsClassifier(n_neighbors= i, weights= 'distance') #Euclidean Distance\n",
    "    knn.fit(train_set_x, train_set_y)\n",
    "    knn_predict= knn.predict(test_set_x)\n",
    "    knn_acc= accuracy_score(test_set_y, knn_predict)\n",
    "    knn_error= 1-knn_acc\n",
    "    euclid_dict[i]= knn_error\n",
    "best_euclid_err = min(euclid_dict.values())\n",
    "mult_k = []\n",
    "for k, v in euclid_dict.items():\n",
    "    if v == best_euclid_err:\n",
    "        mult_k.append(k)\n",
    "best_euclid_k= max(mult_k)\n",
    "print(\"The best k is:\",str(best_euclid_k),\"\\nIt's best error rate is: \"+str(best_euclid_err)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manha_dict= {}\n",
    "for i in range(1, 197, 5):\n",
    "    knn_2= KNeighborsClassifier(n_neighbors= i, weights= 'distance', p=1) #Manhattan Distance\n",
    "    knn_2.fit(train_set_x, train_set_y)\n",
    "    knn_2_predict= knn_2.predict(test_set_x)\n",
    "    knn_2_acc= accuracy_score(test_set_y, knn_2_predict)\n",
    "    knn_2_error= 1-knn_2_acc\n",
    "    manha_dict[i] = knn_2_error\n",
    "best_manha_err = min(manha_dict.values()) \n",
    "mult_k = []\n",
    "for k, v in manha_dict.items():\n",
    "    if v == best_manha_err:\n",
    "        mult_k.append(k)\n",
    "best_manha_k= max(mult_k)\n",
    "print(\"The best k is:\",str(best_manha_k),\"\\nIt's best error rate is: \"+str(best_manha_err)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheb_dict= {}\n",
    "for i in range(1, 197, 5):\n",
    "    knn_3= KNeighborsClassifier(n_neighbors= i, metric= 'chebyshev', weights= 'distance') #Chebyshev Distance\n",
    "    knn_3.fit(train_set_x, train_set_y)\n",
    "    knn_3_predict= knn_3.predict(test_set_x)\n",
    "    knn_3_acc= accuracy_score(test_set_y, knn_3_predict)\n",
    "    knn_3_error= 1-knn_3_acc\n",
    "    cheb_dict[i] = knn_3_error\n",
    "best_cheb_err = min(cheb_dict.values()) \n",
    "mult_k = []\n",
    "for k, v in cheb_dict.items():\n",
    "    if v == best_cheb_err:\n",
    "        mult_k.append(k)\n",
    "best_cheb_k= max(mult_k)\n",
    "print(\"The best k is:\",str(best_cheb_k),\"\\nIt's best error rate is: \"+str(best_cheb_err)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) What is the lowest training error rate you achieved in this homework?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0, because when k= 1 the closest training sample is itself.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
